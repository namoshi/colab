{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7iWKQLVvyu8DVEHlDX1r4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namoshi/colab/blob/master/NLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3-zbHenAv1r",
        "outputId": "92a5c0e5-792f-4ba2-c478-44cc508a314f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (3.8.3)\n",
            "Requirement already satisfied: huggingface==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 2)) (0.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 6)) (4.5.1.48)\n",
            "Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (1.8.1)\n",
            "Requirement already satisfied: torchaudio==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 11)) (0.8.1)\n",
            "Requirement already satisfied: torchinfo==0.0.8 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 12)) (0.0.8)\n",
            "Requirement already satisfied: torchtext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (0.9.1)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 14)) (0.9.1)\n",
            "Requirement already satisfied: transformers==4.3.3 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.3.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.0.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "os.makedirs('./data',exist_ok=True)\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU1ylpCGBMqY",
        "outputId": "2c7256f6-545c-435f-e457-7cf416f5490e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train.csv: 29.5MB [00:00, 86.1MB/s]\n",
            "test.csv: 1.86MB [00:00, 32.1MB/s]                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMeF9HjFBpxN",
        "outputId": "e84fd303-b69a-4537-d401-b516ef2ce3c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,x in zip(range(5),train_dataset):\n",
        "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI48q4iJBujV",
        "outputId": "9ed2f995-4e24-4b41-afa6-838a2d0ff005"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "**Sci/Tech** -> Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "train_dataset = list(train_dataset)\n",
        "test_dataset = list(test_dataset)"
      ],
      "metadata": {
        "id": "xwRykAVqCLXx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "tokenizer('He said: hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e_eUaHDCNWm",
        "outputId": "0541113a-df2d-487d-f807-be62bcebf247"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'said', 'hello']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
      ],
      "metadata": {
        "id": "bl42uWh7CTiJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size if {vocab_size}\")\n",
        "\n",
        "def encode(x):\n",
        "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "encode('I love to play with my words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CALvSxyCj1K",
        "outputId": "a2f52bb1-41ad-4f69-ae93-14327849949a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size if 95812\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[283, 2321, 5, 337, 19, 1301, 2357]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words text representation"
      ],
      "metadata": {
        "id": "pq0tO3e4CsH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGR1ZwhtCtwd",
        "outputId": "22cd5512-6c1a-4f4e-a740-f987ff834fd4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "def to_bow(text,bow_vocab_size=vocab_size):\n",
        "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
        "    for i in encode(text):\n",
        "        if i<bow_vocab_size:\n",
        "            res[i] += 1\n",
        "    return res\n",
        "\n",
        "print(to_bow(train_dataset[0][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PADLhE0UDB8z",
        "outputId": "eb763e0e-0a7c-426f-c1f7-a350454f0cf7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 2.,  ..., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training BoW classifier"
      ],
      "metadata": {
        "id": "HIniOPAFDIeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np \n",
        "\n",
        "# this collate function gets list of batch_size tuples, and needs to \n",
        "# return a pair of label-feature tensors for the whole minibatch\n",
        "def bowify(b):\n",
        "    return (\n",
        "            torch.LongTensor([t[0]-1 for t in b]),\n",
        "            torch.stack([to_bow(t[1]) for t in b])\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
      ],
      "metadata": {
        "id": "g2RLiI27DJ_Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
      ],
      "metadata": {
        "id": "ND_Nbm9iDThf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,features in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out = net(features)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count"
      ],
      "metadata": {
        "id": "B7__UyaPDWX_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch(net,train_loader,epoch_size=15000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqQSVWzWDatG",
        "outputId": "3cede712-ff30-40f0-f6f5-2e69d50e80f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200: acc=0.7871875\n",
            "6400: acc=0.838125\n",
            "9600: acc=0.8533333333333334\n",
            "12800: acc=0.8625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.02628620448651344, 0.8645389125799574)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BiGrams, TriGrams and N-Grams"
      ],
      "metadata": {
        "id": "5hn2Sqz7EChh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "bigram_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
        "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9gkaPF5ED3V",
        "outputId": "b848d8f0-f19f-4e5b-ac16-1ab01c61d176"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    l = tokenizer(line)\n",
        "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
        "    \n",
        "bi_vocab = torchtext.vocab.Vocab(counter, min_freq=1)\n",
        "\n",
        "print(\"Bigram vocabulary length = \",len(bi_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhK0W6dJEJLX",
        "outputId": "5e29cbe9-1635-41d1-9b21-af82251e3f15"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram vocabulary length =  1308844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency Inverse Document Frequency TF-IDF"
      ],
      "metadata": {
        "id": "qMp9-zB0EoWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Q7rXZYEpxN",
        "outputId": "ab199fac-3397-4d5e-e022-fb43682932b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
              "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Represent words with embeddings"
      ],
      "metadata": {
        "id": "PqCDQJOgE2we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
      ],
      "metadata": {
        "id": "Zff3ooLOE4PJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "from torchnlp import *\n",
        "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size = \",vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HblQuwQ9E-w_",
        "outputId": "62a2d20b-4ef0-4422-da2c-e1caf2e673c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Building vocab...\n",
            "Vocab size =  95812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.mean(x,dim=1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "-iwARnXoFFSo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padify(b):\n",
        "    # b is the list of tuples of length batch_size\n",
        "    #   - first element of a tuple = label, \n",
        "    #   - second = feature (text sequence)\n",
        "    # build vectorized sequence\n",
        "    v = [encode(x[1]) for x in b]\n",
        "    # first, compute max length of a sequence in this minibatch\n",
        "    l = max(map(len,v))\n",
        "    return ( # tuple of two tensors - labels and features\n",
        "        torch.LongTensor([t[0]-1 for t in b]),\n",
        "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
      ],
      "metadata": {
        "id": "ZlHMPCkyFLF1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
        "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8vcri1zFQ2q",
        "outputId": "01a8b6aa-e378-47c9-d0a6-8c8f7345b2d1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200: acc=0.65\n",
            "6400: acc=0.69015625\n",
            "9600: acc=0.7096875\n",
            "12800: acc=0.725546875\n",
            "16000: acc=0.7385625\n",
            "19200: acc=0.74875\n",
            "22400: acc=0.7552232142857143\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9403780633947136, 0.7611164427383237)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmbeddingBag Layer and Variable-Length Sequence Representation"
      ],
      "metadata": {
        "id": "O8gzWx2ZFe53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
        "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, text, off):\n",
        "        x = self.embedding(text, off)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "Wgx3p020FgqX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def offsetify(b):\n",
        "    # first, compute data tensor from all sequences\n",
        "    x = [torch.tensor(encode(t[1])) for t in b]\n",
        "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
        "    o = [0] + [len(t) for t in x]\n",
        "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
        "    return ( \n",
        "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
        "        torch.cat(x), # text \n",
        "        o\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
      ],
      "metadata": {
        "id": "CQYRy3vwFlzX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
        "\n",
        "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    loss_fn = loss_fn.to(device)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,text,off in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
        "        out = net(text, off)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count\n",
        "\n",
        "\n",
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GC0yWo5FrMC",
        "outputId": "23923e63-a9a3-46ec-a6c5-08bab0496c57"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200: acc=0.6284375\n",
            "6400: acc=0.679375\n",
            "9600: acc=0.7048958333333334\n",
            "12800: acc=0.720078125\n",
            "16000: acc=0.7331875\n",
            "19200: acc=0.74453125\n",
            "22400: acc=0.7523660714285715\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22.09985304702495, 0.7576775431861804)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Embeddings: Word2Vec"
      ],
      "metadata": {
        "id": "fzqP7xjdF0yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "gm-9mP2jF2Hw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('neural'):\n",
        "    print(f\"{w} -> {p}\")"
      ],
      "metadata": {
        "id": "9x1wGktaF6sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.word_vec('play')[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "WUIy4BgkGGa2",
        "outputId": "e9a607ae-4708-4520-d27d-2ddf27cc0cc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f632b3457e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'play'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
      ],
      "metadata": {
        "id": "2OO4xXSlGHQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Pre-Trained Embeddings in PyTorch"
      ],
      "metadata": {
        "id": "qbWrmBGgGNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
        "\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found = 0,0\n",
        "for i,w in enumerate(vocab.itos):\n",
        "    try:\n",
        "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
        "        found+=1\n",
        "    except:\n",
        "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
        "        not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "KJPQQ1pAGPBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "metadata": {
        "id": "DD9rLjHZGWHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
      ],
      "metadata": {
        "id": "CntX4PBuGaIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vector corresponding to kind-man+woman\n",
        "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
        "# find the index of the closest embedding vector \n",
        "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
        "min_idx = torch.argmin(d)\n",
        "# find the corresponding word\n",
        "vocab.itos[min_idx]"
      ],
      "metadata": {
        "id": "rXLl75CwGaxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def offsetify(b):\n",
        "    # first, compute data tensor from all sequences\n",
        "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
        "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
        "    o = [0] + [len(t) for t in x]\n",
        "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
        "    return ( \n",
        "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
        "        torch.cat(x), # text \n",
        "        o\n",
        "    )"
      ],
      "metadata": {
        "id": "ZpB3OCdFGf0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
        "net.embedding.weight.data = vocab.vectors\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "rX5Az3-sGja-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "metadata": {
        "id": "pUqLPCWJGm5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capture patterns with recurrent neural networks"
      ],
      "metadata": {
        "id": "aqpHNW91G0rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZNOJ37WVGqLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}