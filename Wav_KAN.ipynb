{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7KgHeViIazjQOvB00sdEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namoshi/colab/blob/master/Wav_KAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wev-KAN"
      ],
      "metadata": {
        "id": "c8JdzEEDU8Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from KAN import *"
      ],
      "metadata": {
        "id": "JSmpj-57Vmfd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the wavelet types\n",
        "#wavelet_types = ['mexican_hat', 'morlet', 'dog', 'meyer', 'shannon', 'bump', etc.] #It can include #all wavelet types\n",
        "wavelet_types = ['mexican_hat', 'morlet', 'dog', 'shannon']\n",
        "\n",
        "# Loading MNIST data set\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "valset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Trials and Epochs (epochs per trial)\n",
        "trials = 5\n",
        "epochs_per_trial = 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c39xRadNVs0s",
        "outputId": "06a824c4-7520-491e-dd1d-4dc3c051dc0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 3522397.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134852.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1269825.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3818506.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzhKxaxdVvd7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QteOT3M-U6dW",
        "outputId": "5b7a92cf-dc40-416f-b62a-0ed71409c25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wavelet is mexican_hat\n",
            "Trial is 0\n",
            "Trial is 1\n",
            "Trial is 2\n",
            "Trial is 3\n",
            "Trial is 4\n",
            "Results saved to mexican_hat_results.xlsx.\n",
            "Wavelet is morlet\n",
            "Trial is 0\n",
            "Trial is 1\n",
            "Trial is 2\n",
            "Trial is 3\n",
            "Trial is 4\n"
          ]
        }
      ],
      "source": [
        "# Looping over each wavelet type\n",
        "for wavelet in wavelet_types:\n",
        "    all_train_losses, all_train_accuracies = [], []\n",
        "    all_val_losses, all_val_accuracies = [], []\n",
        "    print(f'Wavelet is {wavelet}')\n",
        "    #For a specified number of trials\n",
        "    for trial in range(trials):\n",
        "        print(f'Trial is {trial}')\n",
        "        # Define model, optimizer, scheduler for each trial\n",
        "        model = KAN([28 * 28, 32, 10], wavelet_type=wavelet)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        trial_train_losses, trial_val_losses = [], []\n",
        "        trial_train_accuracies, trial_val_accuracies = [], []\n",
        "        #For a specified number of epchs\n",
        "        for epoch in range(epochs_per_trial):\n",
        "            # Training\n",
        "            train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "            model.train()\n",
        "            #for images, labels in tqdm(trainloader):\n",
        "            for images, labels in trainloader:\n",
        "                images = images.view(-1, 28 * 28).to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            train_loss /= len(trainloader)\n",
        "            train_acc = 100 * train_correct / train_total\n",
        "            trial_train_losses.append(train_loss)\n",
        "            trial_train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, labels in valloader:\n",
        "                    images = images.view(-1, 28 * 28).to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "            val_loss /= len(valloader)\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "            trial_val_losses.append(val_loss)\n",
        "            trial_val_accuracies.append(val_acc)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "        #collecting statistics\n",
        "        all_train_losses.append(trial_train_losses)\n",
        "        all_train_accuracies.append(trial_train_accuracies)\n",
        "        all_val_losses.append(trial_val_losses)\n",
        "        all_val_accuracies.append(trial_val_accuracies)\n",
        "    # Average results across trials and write to Excel\n",
        "    avg_train_losses = pd.DataFrame(all_train_losses).mean().tolist()\n",
        "    avg_train_accuracies = pd.DataFrame(all_train_accuracies).mean().tolist()\n",
        "    avg_val_losses = pd.DataFrame(all_val_losses).mean().tolist()\n",
        "    avg_val_accuracies = pd.DataFrame(all_val_accuracies).mean().tolist()\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Epoch': range(1, epochs_per_trial + 1),\n",
        "        'Train Loss': avg_train_losses,\n",
        "        'Train Accuracy': avg_train_accuracies,\n",
        "        'Validation Loss': avg_val_losses,\n",
        "        'Validation Accuracy': avg_val_accuracies\n",
        "    })\n",
        "    # Saving the results\n",
        "    # Saving the results to an Excel file named after the wavelet type\n",
        "    file_name = f'{wavelet}_results.xlsx'\n",
        "    results_df.to_excel(file_name, index=False)\n",
        "\n",
        "    print(f\"Results saved to {file_name}.\")"
      ]
    }
  ]
}