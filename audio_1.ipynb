{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEH/JYmpsPN8a2VJJp9+mQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7edee8d5caf40dabf63fc811915fb91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_35aaf74dfa07448bb2b5fde2bcac120b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b460245db9f141d2954ab7ea35f047ab",
              "IPY_MODEL_de796473206a4ee4ada9a799b94014ff",
              "IPY_MODEL_13edb3890fcc46a88ac20b60e85e434b"
            ]
          }
        },
        "35aaf74dfa07448bb2b5fde2bcac120b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b460245db9f141d2954ab7ea35f047ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6325175c8d134793920182f388cfa1ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 92%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65ae1a95f6a14c4498918d9122bb37a1"
          }
        },
        "de796473206a4ee4ada9a799b94014ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4bc5b0ad7be5480d8407e610b23765b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2428923189,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2244509696,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a32d0d863434dea8be9b45c9b8eab65"
          }
        },
        "13edb3890fcc46a88ac20b60e85e434b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_11f4ba6029524ebc9dbf4b9a5a7a9e04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.09G/2.26G [00:22&lt;00:05, 35.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76a507600c404f5a8597c33742b6f53d"
          }
        },
        "6325175c8d134793920182f388cfa1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65ae1a95f6a14c4498918d9122bb37a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4bc5b0ad7be5480d8407e610b23765b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a32d0d863434dea8be9b45c9b8eab65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11f4ba6029524ebc9dbf4b9a5a7a9e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76a507600c404f5a8597c33742b6f53d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namoshi/colab/blob/master/audio_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understand audio data and concepts"
      ],
      "metadata": {
        "id": "agVlc_tsPgyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kWqbtl5HPf6L"
      },
      "outputs": [],
      "source": [
        "# import the packages\n",
        "import os\n",
        "import torchaudio\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_dir = os.getcwd()\n",
        "folder = 'data'\n",
        "print(f'Data directory will be: {default_dir}/{folder}')\n",
        "\n",
        "if os.path.isdir(folder):\n",
        "    print(\"Data folder exists.\")\n",
        "else:\n",
        "    print(\"Creating folder.\")\n",
        "    os.mkdir(folder) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKwZvn9HPneV",
        "outputId": "2f043ddf-8d4e-4173-ce32-bac62a653b08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory will be: /content/data\n",
            "Creating folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset_speechcommands = torchaudio.datasets.SPEECHCOMMANDS(f'./{folder}/', download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48,
          "referenced_widgets": [
            "a7edee8d5caf40dabf63fc811915fb91",
            "35aaf74dfa07448bb2b5fde2bcac120b",
            "b460245db9f141d2954ab7ea35f047ab",
            "de796473206a4ee4ada9a799b94014ff",
            "13edb3890fcc46a88ac20b60e85e434b",
            "6325175c8d134793920182f388cfa1ee",
            "65ae1a95f6a14c4498918d9122bb37a1",
            "4bc5b0ad7be5480d8407e610b23765b0",
            "1a32d0d863434dea8be9b45c9b8eab65",
            "11f4ba6029524ebc9dbf4b9a5a7a9e04",
            "76a507600c404f5a8597c33742b6f53d"
          ]
        },
        "id": "xkvnsyktPwdV",
        "outputId": "5b5d1fd0-cdec-4dd1-d421-c165b8ba0f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7edee8d5caf40dabf63fc811915fb91",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/2.26G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(f'./{folder}/SpeechCommands/speech_commands_v0.02/')\n",
        "labels = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
        "# back to default directory\n",
        "os.chdir(default_dir)\n",
        "print(f'Total Labels: {len(labels)}')\n",
        "print(f'Label Names: {labels}')"
      ],
      "metadata": {
        "id": "qUOtlFfvP7gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"./data/SpeechCommands/speech_commands_v0.02/yes/00f0204f_nohash_0.wav\"\n",
        "waveform, sample_rate = torchaudio.load(filepath=filename, num_frames=3)\n",
        "print(f'waveform tensor:{waveform}')\n",
        "waveform, sample_rate = torchaudio.load(filepath=filename, num_frames=3, frame_offset =2)\n",
        "print(waveform)\n",
        "waveform, sample_rate = torchaudio.load(filepath=filename)\n",
        "print(waveform)"
      ],
      "metadata": {
        "id": "F4t6HL1oQGrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the waveform"
      ],
      "metadata": {
        "id": "7bM5xI7xQLXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_audio(filename):\n",
        "    waveform, sample_rate = torchaudio.load(filename)\n",
        "\n",
        "    print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "    print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(waveform.t().numpy())\n",
        "\n",
        "    return waveform, sample_rate"
      ],
      "metadata": {
        "id": "FaucRRczQMrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"./data/SpeechCommands/speech_commands_v0.02/yes/00f0204f_nohash_0.wav\"\n",
        "waveform, sample_rate = plot_audio(filename)\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "metadata": {
        "id": "Osjm-xVBQUJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"./data/SpeechCommands/speech_commands_v0.02/no/0b40aa8e_nohash_0.wav\"\n",
        "waveform, sample_rate = plot_audio(filename)\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "metadata": {
        "id": "fDpO0DbcReMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio transforms and visualizations"
      ],
      "metadata": {
        "id": "RYoaLqsHR0gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "gQ9XP_suR2Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio_files(path: str, label:str):\n",
        "\n",
        "    dataset = []\n",
        "    walker = sorted(str(p) for p in Path(path).glob(f'*.wav'))\n",
        "\n",
        "    for i, file_path in enumerate(walker):\n",
        "        path, filename = os.path.split(file_path)\n",
        "        speaker, _ = os.path.splitext(filename)\n",
        "        speaker_id, utterance_number = speaker.split(\"_nohash_\")\n",
        "        utterance_number = int(utterance_number)\n",
        "    \n",
        "        # Load audio\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
        "        \n",
        "    return dataset"
      ],
      "metadata": {
        "id": "R7J4aVhmSGl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset_speechcommands_yes = load_audio_files('./data/SpeechCommands/speech_commands_v0.02/yes', 'yes')\n",
        "trainset_speechcommands_no = load_audio_files('./data/SpeechCommands/speech_commands_v0.02/no', 'no')"
      ],
      "metadata": {
        "id": "uFWFQEqgSMOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Length of yes dataset: {len(trainset_speechcommands_yes)}')\n",
        "print(f'Length of no dataset: {len(trainset_speechcommands_no)}')"
      ],
      "metadata": {
        "id": "dKDuRwDuSRy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader_yes = torch.utils.data.DataLoader(trainset_speechcommands_yes, batch_size=1,\n",
        "                                            shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "K5-5rllaSj_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader_no = torch.utils.data.DataLoader(trainset_speechcommands_no, batch_size=1,\n",
        "                                            shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "ZEVie0yMSl8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yes_waveform = trainset_speechcommands_yes[0][0]\n",
        "yes_sample_rate = trainset_speechcommands_yes[0][1]\n",
        "print(f'Yes Waveform: {yes_waveform}')\n",
        "print(f'Yes Sample Rate: {yes_sample_rate}')\n",
        "print(f'Yes Label: {trainset_speechcommands_yes[0][2]}')\n",
        "print(f'Yes ID: {trainset_speechcommands_yes[0][3]}')\n",
        "\n",
        "no_waveform = trainset_speechcommands_no[0][0]\n",
        "no_sample_rate = trainset_speechcommands_no[0][1]\n",
        "print(f'No Waveform: {no_waveform}')\n",
        "print(f'No Sample Rate: {no_sample_rate}')\n",
        "print(f'No Label: {trainset_speechcommands_no[0][2]}')\n",
        "print(f'No ID: {trainset_speechcommands_no[0][3]}')"
      ],
      "metadata": {
        "id": "6U6JV0hZSwN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_waveform(waveform, sample_rate, label):\n",
        "    print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, label))\n",
        "    new_sample_rate = sample_rate/10\n",
        "    print(new_sample_rate)\n",
        "    # Resample applies to a single channel, we resample first channel here\n",
        "    channel = 0\n",
        "    waveform_transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n",
        "\n",
        "    print(\"Shape of transformed waveform: {}\".format(waveform_transformed.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(waveform_transformed[0,:].numpy())"
      ],
      "metadata": {
        "id": "Ffez5bFsSx6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_waveform(yes_waveform, yes_sample_rate, 'yes')"
      ],
      "metadata": {
        "id": "gjPVQItoS6cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spectrogram"
      ],
      "metadata": {
        "id": "I17C60MHTEnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_spectrogram(waveform):\n",
        "    spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
        "    #print(spectrogram)\n",
        "    print(\"Shape of spectrogram: {}\".format(spectrogram.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(spectrogram.log2()[0,:,:].numpy(), cmap='gray')\n",
        "    #plt.imsave(f'test/spectrogram_img.png', spectrogram.log2()[0,:,:].numpy(), cmap='gray')\n",
        "  "
      ],
      "metadata": {
        "id": "JPpRJu3DTF-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_spectrogram(yes_waveform)"
      ],
      "metadata": {
        "id": "w2-O0RB9TK83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mel Spectrogram"
      ],
      "metadata": {
        "id": "Anmuv-YjTZjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_melspectrogram(waveform,sample_rate):\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
        "    print(\"Shape of spectrogram: {}\".format(mel_spectrogram.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(mel_spectrogram.log2()[0,:,:].numpy(), cmap='gray')"
      ],
      "metadata": {
        "id": "AUL0xq2RTayo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_melspectrogram(yes_waveform, yes_sample_rate)"
      ],
      "metadata": {
        "id": "b-jxrSlzT7_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mel-frequency cepstral coefficients (MFCC)"
      ],
      "metadata": {
        "id": "VOQBjCFgUF_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mfcc(waveform,sample_rate):\n",
        "    mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate= sample_rate)(waveform)\n",
        "    print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))\n",
        "\n",
        "    plt.figure()\n",
        "    fig1 = plt.gcf()\n",
        "    plt.imshow(mfcc_spectrogram.log2()[0,:,:].numpy(), cmap='gray')\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(mfcc_spectrogram.log2()[0,:,:].numpy())\n",
        "    plt.draw()"
      ],
      "metadata": {
        "id": "7j5P0rWGUHTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_mfcc(no_waveform,  no_sample_rate)"
      ],
      "metadata": {
        "id": "BqfJVE5RUMVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an image from a Spectrogram"
      ],
      "metadata": {
        "id": "SUJSuyW-UWxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_images(trainloader, label_dir):\n",
        "    #make directory\n",
        "    directory = f'./data/spectrograms/{label_dir}/'\n",
        "    if(os.path.isdir(directory)):\n",
        "        print(\"Data exists\")\n",
        "    else:\n",
        "        os.makedirs(directory, mode=0o777, exist_ok=True)\n",
        "        \n",
        "        for i, data in enumerate(trainloader):\n",
        "\n",
        "            waveform = data[0]\n",
        "            sample_rate = data[1][0]\n",
        "            label = data[2]\n",
        "            ID = data[3]\n",
        "\n",
        "            # create transformed waveforms\n",
        "            spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)     \n",
        "            \n",
        "            fig = plt.figure()\n",
        "            plt.imsave(f'./data/spectrograms/{label_dir}/spec_img{i}.png', spectrogram_tensor[0].log2()[0,:,:].numpy(), cmap='gray')\n"
      ],
      "metadata": {
        "id": "j8pXZTM7UYCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mfcc_images(trainloader, label_dir):\n",
        "    #make directory\n",
        "    os.makedirs(f'./data/mfcc_spectrograms/{label_dir}/', mode=0o777, exist_ok=True)\n",
        "    \n",
        "    for i, data in enumerate(trainloader):\n",
        "\n",
        "        waveform = data[0]\n",
        "        sample_rate = data[1][0]\n",
        "        label = data[2]\n",
        "        ID = data[3]\n",
        "        \n",
        "        mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate= sample_rate)(waveform)\n",
        "\n",
        "        plt.figure()\n",
        "        fig1 = plt.gcf()\n",
        "        plt.imshow(mfcc_spectrogram[0].log2()[0,:,:].numpy(), cmap='gray')\n",
        "        plt.draw()\n",
        "        fig1.savefig(f'./data/mfcc_spectrograms/{label_dir}/spec_img{i}.png', dpi=100)\n",
        " \n",
        "        #spectorgram_train.append([spectrogram_tensor, label, sample_rate, ID])"
      ],
      "metadata": {
        "id": "Qx87fVTZUerO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_images(trainloader_yes, 'yes')\n",
        "create_images(trainloader_no, 'no')"
      ],
      "metadata": {
        "id": "q3MGQ36GUl0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the speech model"
      ],
      "metadata": {
        "id": "8rNEZT59VHX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "ybByUlAwVI5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './data/spectrograms' #looking in subfolder train\n",
        "\n",
        "yes_no_dataset = datasets.ImageFolder(\n",
        "    root=data_path,\n",
        "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
        "                                  transforms.ToTensor()\n",
        "                                  ])\n",
        ")\n",
        "print(yes_no_dataset)\n",
        "print(yes_no_dataset[5][0].size())"
      ],
      "metadata": {
        "id": "TNr6cW53WQTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data to test and train\n",
        "#use 80% to train\n",
        "train_size = int(0.8 * len(yes_no_dataset))\n",
        "test_size = len(yes_no_dataset) - train_size\n",
        "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
        "\n",
        "print(len(yes_no_train_dataset))\n",
        "print(len(yes_no_test_dataset))"
      ],
      "metadata": {
        "id": "U5HT5O_4WXGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    yes_no_train_dataset,\n",
        "    batch_size=15,\n",
        "    num_workers=2,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    yes_no_test_dataset,\n",
        "    batch_size=15,\n",
        "    num_workers=2,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "7q1o5d-AWZix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader.dataset[0][0][0][0]"
      ],
      "metadata": {
        "id": "fMoPj-1rWg_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "metadata": {
        "id": "DtqXCujGWlfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rZqT2ITjQV5E"
      }
    }
  ]
}