{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFYnLim65OJZiN2TOyiZmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namoshi/colab/blob/master/NLP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Represent words with embeddings"
      ],
      "metadata": {
        "id": "PqCDQJOgE2we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
      ],
      "metadata": {
        "id": "Zff3ooLOE4PJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "from torchnlp import *\n",
        "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size = \",vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HblQuwQ9E-w_",
        "outputId": "90616b97-4e61-425e-92a1-0fb8d1e9b5d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Building vocab...\n",
            "Vocab size =  95812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.mean(x,dim=1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "-iwARnXoFFSo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padify(b):\n",
        "    # b is the list of tuples of length batch_size\n",
        "    #   - first element of a tuple = label, \n",
        "    #   - second = feature (text sequence)\n",
        "    # build vectorized sequence\n",
        "    v = [encode(x[1]) for x in b]\n",
        "    # first, compute max length of a sequence in this minibatch\n",
        "    l = max(map(len,v))\n",
        "    return ( # tuple of two tensors - labels and features\n",
        "        torch.LongTensor([t[0]-1 for t in b]),\n",
        "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
      ],
      "metadata": {
        "id": "ZlHMPCkyFLF1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
        "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8vcri1zFQ2q",
        "outputId": "b964f3b4-adcb-4a25-b33c-d45433093939"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200: acc=0.6440625\n",
            "6400: acc=0.67984375\n",
            "9600: acc=0.703125\n",
            "12800: acc=0.722578125\n",
            "16000: acc=0.7339375\n",
            "19200: acc=0.7446875\n",
            "22400: acc=0.7554910714285714\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.960481474053303, 0.7596369161868202)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EmbeddingBag Layer and Variable-Length Sequence Representation"
      ],
      "metadata": {
        "id": "O8gzWx2ZFe53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
        "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, text, off):\n",
        "        x = self.embedding(text, off)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "Wgx3p020FgqX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def offsetify(b):\n",
        "    # first, compute data tensor from all sequences\n",
        "    x = [torch.tensor(encode(t[1])) for t in b]\n",
        "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
        "    o = [0] + [len(t) for t in x]\n",
        "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
        "    return ( \n",
        "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
        "        torch.cat(x), # text \n",
        "        o\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
      ],
      "metadata": {
        "id": "CQYRy3vwFlzX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
        "\n",
        "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    loss_fn = loss_fn.to(device)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,text,off in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
        "        out = net(text, off)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count\n",
        "\n",
        "\n",
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GC0yWo5FrMC",
        "outputId": "143cffdd-eefb-4d11-e1de-65d51514ecc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3200: acc=0.643125\n",
            "6400: acc=0.685625\n",
            "9600: acc=0.7111458333333334\n",
            "12800: acc=0.725546875\n",
            "16000: acc=0.7358125\n",
            "19200: acc=0.7458854166666666\n",
            "22400: acc=0.7540178571428572\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22.84136826215611, 0.7594369801663468)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Embeddings: Word2Vec"
      ],
      "metadata": {
        "id": "fzqP7xjdF0yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "gm-9mP2jF2Hw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('neural'):\n",
        "    print(f\"{w} -> {p}\")"
      ],
      "metadata": {
        "id": "9x1wGktaF6sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.word_vec('play')[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "WUIy4BgkGGa2",
        "outputId": "e9a607ae-4708-4520-d27d-2ddf27cc0cc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f632b3457e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'play'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
      ],
      "metadata": {
        "id": "2OO4xXSlGHQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Pre-Trained Embeddings in PyTorch"
      ],
      "metadata": {
        "id": "qbWrmBGgGNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
        "\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found = 0,0\n",
        "for i,w in enumerate(vocab.itos):\n",
        "    try:\n",
        "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
        "        found+=1\n",
        "    except:\n",
        "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
        "        not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "KJPQQ1pAGPBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "metadata": {
        "id": "DD9rLjHZGWHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
      ],
      "metadata": {
        "id": "CntX4PBuGaIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vector corresponding to kind-man+woman\n",
        "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
        "# find the index of the closest embedding vector \n",
        "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
        "min_idx = torch.argmin(d)\n",
        "# find the corresponding word\n",
        "vocab.itos[min_idx]"
      ],
      "metadata": {
        "id": "rXLl75CwGaxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def offsetify(b):\n",
        "    # first, compute data tensor from all sequences\n",
        "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
        "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
        "    o = [0] + [len(t) for t in x]\n",
        "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
        "    return ( \n",
        "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
        "        torch.cat(x), # text \n",
        "        o\n",
        "    )"
      ],
      "metadata": {
        "id": "ZpB3OCdFGf0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
        "net.embedding.weight.data = vocab.vectors\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "rX5Az3-sGja-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
        "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
      ],
      "metadata": {
        "id": "pUqLPCWJGm5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}